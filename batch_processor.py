# agent-api/batch_processor.py
from dotenv import load_dotenv
import time
import json
from uuid import UUID
# Importa o especialista em an√°lise de lote que definimos
from agents.knowledge_builder_definitions import batch_analysis_agent

# Importa os reposit√≥rios, nossa √∫nica camada de acesso a dados
from repositories.chamados_repository import ChamadosRepository
from repositories.knowledge_repository import KnowledgeRepository
from repositories.log_repository import LogRepository

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

def main():
    """
    Fun√ß√£o principal que orquestra o processamento em massa de chamados,
    utilizando uma arquitetura de reposit√≥rios para acesso a dados e logging robusto.
    """
    # --- 1. SETUP INICIAL ---
    start_time_total = time.time()
    print("üöÄ INICIANDO PROCESSAMENTO EM MASSA (ARQUITETURA DE REPOSIT√ìRIO) üöÄ")

    # Instanciamos os reposit√≥rios que o worker ir√° usar
    chamados_repo = ChamadosRepository()
    knowledge_repo = KnowledgeRepository()
    log_repo = LogRepository()

    # Par√¢metros de execu√ß√£o
    BATCH_SIZE = 1  # Processamento individual para melhor controle
    MAX_RETRIES = 3  # N√∫mero m√°ximo de tentativas por chamado
    count_processed = 0

    print("\nüîß Configura√ß√£o:")
    print(f"   ‚Üí Tamanho do lote: {BATCH_SIZE}")
    print(f"   ‚Üí M√°ximo de tentativas: {MAX_RETRIES}")

    total_succeeded = 0
    total_failed = 0
    job_id = None
    tickets_found = 0

    try:
        # --- 2. CRIA√á√ÉO DO JOB DE LOG ---
        initial_tickets = chamados_repo.get_unprocessed_tickets(limit=1)
        if not initial_tickets:
            print("üèÅ Nenhum chamado para processar. Encerrando.")
            return

        tickets_found = len(initial_tickets)
        job_id = log_repo.create_job(tickets_found=tickets_found, batch_size=BATCH_SIZE, job_type='knowledge')
        c = 0
        # --- 3. LOOP DE PROCESSAMENTO EM LOTES ---
        while True or count_processed == 1:

            ticket_ids_batch = chamados_repo.get_unprocessed_tickets(limit=BATCH_SIZE)
            if not ticket_ids_batch:
                print("üèÅ Todos os chamados foram processados.")
                break

            print(f"\n--- Processando lote de {len(ticket_ids_batch)} chamados (Job ID: {job_id}) ---")

            batch_start_time = time.time()
            log_entries = []

            try:
                # Fase 1: Gera√ß√£o dos dossi√™s
                print("\nüìë Gerando dossi√™s para an√°lise...")
                dossiers_data = chamados_repo.generate_dossiers_for_tickets(ticket_ids_batch)
                if not dossiers_data:
                    raise ValueError("Nenhum dossi√™ foi gerado para os chamados selecionados")

                # Fase 2: Prepara√ß√£o dos dados para o LLM
                llm_input = json.dumps([item['dossier_text'] for item in dossiers_data])
                input_size = len(llm_input)
                print(f"üîç Enviando para an√°lise: {input_size:,} caracteres")
                if input_size < 10:
                    raise ValueError("Texto do dossi√™ muito curto para an√°lise")

                # Fase 3: Execu√ß√£o do agente e valida√ß√£o da resposta
                print("ü§ñ Executando an√°lise com IA...")

                # Configura√ß√£o de metadados para rastreamento
                execution_metadata = {
                    "batch_size": BATCH_SIZE,
                    "job_id": str(job_id),
                    "tickets": [str(id) for id in ticket_ids_batch],
                    "input_size": input_size,
                    "processing_version": 1
                }

                # Tentar executar o agente com retries
                max_retries = 3
                raw_result = None
                last_error = None

                for attempt in range(max_retries):
                    try:
                        print(f"\nüîÑ Tentativa {attempt + 1}/{max_retries}")
                        raw_result = batch_analysis_agent.run(
                            input=llm_input,
                            session_id=str(job_id),
                            metadata=execution_metadata,
                            stream=False,
                            session_state={  # Cache de estado da sess√£o
                            "last_successful_batch": None,
                                "current_batch_index": count_processed
                            },
                            timeout=120  # 2 minute timeout
                        )

                        if raw_result is not None and hasattr(raw_result, 'content'):
                            break  # Success, exit retry loop

                    except Exception as retry_error:
                        if attempt < 2:  # Don't sleep on last attempt
                            time.sleep(2 ** attempt)  # Exponential backoff: 1s, 2s, 4s
                            print(f"Tentativa {attempt + 1} falhou, tentando novamente...")
                            continue
                        raise  # Re-raise on final attempt

                    # Debug do resultado bruto da IA
                    print("\nüîç Analisando resposta da IA:")
                    print(f"   Tipo: {type(raw_result).__name__}")
                    if hasattr(raw_result, 'content'):
                        print("   ‚Üí Conte√∫do encontrado em raw_result.content")
                        print(f"   ‚Üí Tipo do conte√∫do: {type(raw_result.content).__name__}")
                        print(f"   ‚Üí Valor: {str(raw_result.content)[:1000]}")
                        analysis_result = raw_result.content
                    elif isinstance(raw_result, dict):
                        print("   ‚Üí Resultado j√° √© um dicion√°rio")
                        print(f"   ‚Üí Valor: {str(raw_result)[:1000]}")
                        analysis_result = raw_result
                    else:
                        print("\n‚ùå Erro: Formato de resposta inesperado")
                        print(f"   Tipo: {type(raw_result).__name__}")
                        print(f"   Valor: {str(raw_result)[:1000]}")

                        # Tentar extrair mais informa√ß√µes se poss√≠vel
                        print("\nüî¨ Tentando extrair mais informa√ß√µes:")
                        for attr in dir(raw_result):
                            if not attr.startswith('_'):
                                try:
                                    value = getattr(raw_result, attr)
                                    print(f"   ‚Üí {attr}: {str(value)[:200]}")
                                except:
                                    continue

                        raise ValueError("Formato de resposta inv√°lido")

                    # Improved response handling
                    if raw_result is None:
                        raise ValueError("Resposta vazia do agente")

                    # Handle string response (common with Gemini)
                    if isinstance(raw_result.content, str):
                        try:
                            # Clean up the response if it's a markdown code block
                            content = raw_result.content.strip()
                            if content.startswith('```json'):
                                content = content[7:].strip()
                            if content.endswith('```'):
                                content = content[:-3].strip()
                            analysis_result = json.loads(content)
                        except json.JSONDecodeError as e:
                            print(f"\n‚ùå Erro ao decodificar JSON: {e}")
                            print(f"Resposta bruta:\n{raw_result.content}")
                            raise ValueError(f"Falha ao parsear JSON da resposta: {e}")
                    else:
                        analysis_result = raw_result.content

                    if not isinstance(analysis_result, dict):
                        raise ValueError(f"Resposta inv√°lida, esperava dict mas recebi {type(analysis_result)}")

                    if 'records' not in analysis_result:
                        print("\n‚ùå Erro: Resposta n√£o cont√©m registros v√°lidos")
                        print(f"   Conte√∫do: {str(analysis_result)[:500]}")
                        raise ValueError("Estrutura de resposta inv√°lida: 'records' n√£o encontrado")

                    knowledge_records = analysis_result['records']
                    if not knowledge_records:
                        raise ValueError("Lista de registros vazia")

                    print(f"\n‚úÖ An√°lise conclu√≠da com sucesso!")
                    print(f"   ‚Üí {len(knowledge_records)} registros gerados")
                    if knowledge_records:
                        print("   ‚Üí Campos encontrados:", list(knowledge_records[0].keys()))
                    except Exception as e:
                        print(f"‚ùå Erro durante a an√°lise do lote: {str(e)}")
                        raise

                if len(knowledge_records) != len(dossiers_data):
                    raise ValueError(f"Inconsist√™ncia de contagem da LLM. Entrada: {len(dossiers_data)}, Sa√≠da: {len(knowledge_records)}")

                knowledge_to_save = [record.model_dump() for record in knowledge_records]

                saved_ids = knowledge_repo.save_batch(knowledge_to_save)
                id_map = {item['ticket_id']: saved_id for item, saved_id in zip(knowledge_to_save, saved_ids)}
                chamados_repo.mark_tickets_as_processed([item['ticket_id'] for item in knowledge_to_save])

                for item in knowledge_to_save:
                    ticket_id = item['ticket_id']
                    duration = int((time.time() - batch_start_time) * 1000 / len(ticket_ids_batch))
                    log_entries.append({
                        "ticket_id": ticket_id,
                        "knowledge_base_id": id_map.get(ticket_id),
                        "status": "SUCCESS",
                        "duration_ms": duration,
                        "error_message": None
                    })
                total_succeeded += len(knowledge_to_save)
                print(f"  -> ‚úÖ Lote processado com sucesso em {time.time() - batch_start_time:.2f} segundos.")

            except Exception as e:
                error_msg = f"Erro no processamento do lote: {str(e)}"
                print(f"  -> ‚ùå {error_msg}")
                duration = int((time.time() - batch_start_time) * 1000)
                for ticket_id in ticket_ids_batch:
                    log_entries.append({
                        "ticket_id": ticket_id,
                        "knowledge_base_id": None,
                        "status": "BATCH_FAILURE",
                        "duration_ms": duration,
                        "error_message": error_msg
                    })
                total_failed += len(ticket_ids_batch)

            finally:
                count_processed += 1
                if log_entries:
                    log_repo.log_batch_details(job_id, log_entries)

    except Exception as e:
        error_summary = f"Erro fatal no worker: {e}"
        print(f"\nüö® {error_summary} üö®")
        if job_id:
            log_repo.update_job_summary(
                job_id, "FAILED", total_succeeded, total_failed, error_summary
            )

    finally:
        # --- 4. FINALIZA√á√ÉO E SUM√ÅRIO DO JOB ---
        end_time_total = time.time()
        total_time = end_time_total - start_time_total

        print("\n" + "="*60)
        print("üèÅ RESUMO DO PROCESSAMENTO EM MASSA üèÅ")
        print("="*60)

        if job_id:
            final_status = "COMPLETED" if 'e' not in locals() or not isinstance(e, Exception) else "FAILED"
            log_repo.update_job_summary(
                job_id, final_status, total_succeeded, total_failed
            )

            print(f"\nüìä Estat√≠sticas:")
            print(f"   ‚Üí ID do Job: {job_id}")
            print(f"   ‚Üí Status: {'‚úÖ Conclu√≠do' if final_status == 'COMPLETED' else '‚ùå Falhou'}")
            print(f"   ‚Üí Chamados processados com sucesso: {total_succeeded}")
            print(f"   ‚Üí Chamados com falha: {total_failed}")
            print(f"   ‚Üí Taxa de sucesso: {(total_succeeded/(total_succeeded+total_failed)*100 if total_succeeded+total_failed > 0 else 0):.1f}%")
            print(f"   ‚Üí Tempo total de execu√ß√£o: {total_time:.1f} segundos")

            if total_succeeded > 0:
                print(f"   ‚Üí M√©dia de tempo por chamado: {(total_time/total_succeeded):.1f} segundos")

        print(f"  - Total de chamados encontrados: {tickets_found}")
        print(f"  - ‚úÖ Sucessos: {total_succeeded}")
        print(f"  - ‚ùå Falhas: {total_failed}")
        print(f"  - ‚è±Ô∏è Tempo total: {end_time_total - start_time_total:.2f} segundos")

if __name__ == "__main__":
    main()
